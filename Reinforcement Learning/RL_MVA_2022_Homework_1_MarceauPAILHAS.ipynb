{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAE8kMxe6E6k"
      },
      "source": [
        "# MVA - Homework 1 - Reinforcement Learning (2022/2023)\n",
        "\n",
        "**Name:** PAILHAS Marceau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY4MH0nU637o"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "* The deadline is **November 10 at 11:59 pm (Paris time).**\n",
        "\n",
        "* By doing this homework you agree to the late day policy, collaboration and misconduct rules reported on [Piazza](https://piazza.com/class/l4y5ubadwj64mb/post/6).\n",
        "\n",
        "* **Mysterious or unsupported answers will not receive full credit**. A correct answer, unsupported by calculations, explanation, or algebraic work will receive no credit; an incorrect answer supported by substantially correct calculations and explanations might still receive partial credit.\n",
        "\n",
        "* Answers should be provided in **English**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB__2uUC5U1r"
      },
      "source": [
        "# Colab setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XNj1_VZ2FGJ"
      },
      "source": [
        "from IPython import get_ipython\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  # install rlberry library\n",
        "  !pip install git+https://github.com/rlberry-py/rlberry.git@mva2021#egg=rlberry[default] > /dev/null 2>&1\n",
        "\n",
        "  # install ffmpeg-python for saving videos\n",
        "  !pip install ffmpeg-python > /dev/null 2>&1\n",
        "\n",
        "  # packages required to show video\n",
        "  !pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "  print(\"Libraries installed, please restart the runtime!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8F7RiPXjutB"
      },
      "source": [
        "# Create directory for saving videos\n",
        "!mkdir videos > /dev/null 2>&1\n",
        "\n",
        "# Initialize display and import function to show videos\n",
        "import rlberry.colab_utils.display_setup\n",
        "from rlberry.colab_utils.display_setup import show_video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KISV44N_nCNm"
      },
      "source": [
        "# Useful libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4hKBRTCh6Gk"
      },
      "source": [
        "# Preparation\n",
        "\n",
        "In the coding exercises, you will use a *grid-world* MDP, which is represented in Python using the interface provided by the [Gym](https://gym.openai.com/) library. The cells below show how to interact with this MDP and how to visualize it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "514mHDeQooKa"
      },
      "source": [
        "from rlberry.envs import GridWorld\n",
        "\n",
        "def get_env():\n",
        "  \"\"\"Creates an instance of a grid-world MDP.\"\"\"\n",
        "  env = GridWorld(\n",
        "      nrows=5,\n",
        "      ncols=7,\n",
        "      reward_at = {(0, 6):1.0},\n",
        "      walls=((0, 4), (1, 4), (2, 4), (3, 4)),\n",
        "      success_probability=0.9,\n",
        "      terminal_states=((0, 6),)\n",
        "  )\n",
        "  return env\n",
        "\n",
        "def render_policy(env, policy=None, horizon=50):\n",
        "  \"\"\"Visualize a policy in an environment\n",
        "\n",
        "  Args:\n",
        "    env: GridWorld\n",
        "        environment where to run the policy\n",
        "    policy: np.array\n",
        "        matrix mapping states to action (Ns).\n",
        "        If None, runs random policy.\n",
        "    horizon: int\n",
        "        maximum number of timesteps in the environment.\n",
        "  \"\"\"\n",
        "  env.enable_rendering()\n",
        "  state = env.reset()                       # get initial state\n",
        "  for timestep in range(horizon):\n",
        "      if policy is None:\n",
        "        action = env.action_space.sample()  # take random actions\n",
        "      else:\n",
        "        action = policy[state]\n",
        "      next_state, reward, is_terminal, info = env.step(action)\n",
        "      state = next_state\n",
        "      if is_terminal:\n",
        "        break\n",
        "  # save video and clear buffer\n",
        "  env.save_video('./videos/gw.mp4', framerate=5)\n",
        "  env.clear_render_buffer()\n",
        "  env.disable_rendering()\n",
        "  # show video\n",
        "  show_video('./videos/gw.mp4')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQAHUBw_ifMI"
      },
      "source": [
        "# Create an environment and visualize it\n",
        "env = get_env()\n",
        "render_policy(env)  # visualize random policy\n",
        "\n",
        "# The reward function and transition probabilities can be accessed through\n",
        "# the R and P attributes:\n",
        "print(f\"Shape of the reward array = (S, A) = {env.R.shape}\")\n",
        "print(f\"Shape of the transition array = (S, A, S) = {env.P.shape}\")\n",
        "print(f\"Reward at (s, a) = (1, 0): {env.R[1, 0]}\")\n",
        "print(f\"Prob[s\\'=2 | s=1, a=0]: {env.P[1, 0, 2]}\")\n",
        "print(f\"Number of states and actions: {env.Ns}, {env.Na}\")\n",
        "\n",
        "# The states in the griworld correspond to (row, col) coordinates.\n",
        "# The environment provides a mapping between (row, col) and the index of\n",
        "# each state:\n",
        "print(f\"Index of state (1, 0): {env.coord2index[(1, 0)]}\")\n",
        "print(f\"Coordinates of state 5: {env.index2coord[5]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibGD_3I89CNu"
      },
      "source": [
        "# Part 1 - Dynamic Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR7h5won9NQY"
      },
      "source": [
        "## Question 1.1\n",
        "\n",
        "Consider a general MDP with a discount factor of $\\gamma < 1$. Assume that the horizon is infinite (so there is no termination). A policy $\\pi$ in this MDP\n",
        "induces a value function $V^\\pi$. Suppose an affine transformation is applied to the reward, what is\n",
        "the new value function? Is the optimal policy preserved?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "313W4K3B_LtN"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "We apply an affine transformation to the reward, this means that $\\forall (a,s)\\in A \\times S,\\text{ }  r'(s,a) = c \\times r(s,a) + d $.\n",
        "\n",
        "<br>\n",
        "\n",
        "Using the definition of the value function, $V^\\pi$ is transformed in $V'^\\pi$:\n",
        "$$\\begin{split}\\forall s \\in S,\\text{ } V'^\\pi (s) &= E\\left[ \\sum_{t=0}^\\infty \\gamma^t r'(s,a)|s_0 = s; a_t \\sim d_t(h_t), \\pi \\right] & \\text{ definition of $V^\\pi$}\n",
        "\\\\ &=E\\left[ \\sum_{t=0}^\\infty \\gamma^t  (c \\times r(s,a) + d)|s_0 = s; a_t \\sim d_t(h_t), \\pi \\right] & \\text{ affine transformation of $r(s,a)$}\n",
        "\\\\ &=c \\times E\\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)  \\right] +d \\times E\\left[ \\sum_{t=0}^\\infty \\gamma^t \\right] & \\text{ linearity of the expected value}\\\\\n",
        "&= c \\times V^\\pi(s)+  \\frac{d}{1- \\gamma} & \\text{ definition of $V^\\pi$ and $E[cst]=cst$ }\n",
        "\\end{split} $$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For a given affine transformation of $r$ into $c\\times r +d$, the value function $V^\\pi$ is tranformed in $c \\times V^\\pi(s)+  \\frac{d}{1- \\gamma}$.\n",
        "\n",
        "<br>\n",
        "\n",
        "For a striclty positive $c$, we have $\\max_{\\pi}V'^\\pi = c \\times \\max_{\\pi}(V^\\pi) +\\frac{d}{1- \\gamma}$; <br>\n",
        "This means that an optimal policy that maximizes $V^\\pi$ also maximizes $V'^\\pi$. For a strictly positive c, the optimality of a policy is preserved.\n",
        "\n",
        "<br>\n",
        "\n",
        "If $c=0$, we have $\\max_{\\pi}V'^\\pi = \\frac{d}{1- \\gamma}$; <br>\n",
        "All policies maximize $V'^\\pi$, which is not the case for $V^\\pi$. So the optimality is not preserved.<br>\n",
        " For $c<0$, $ ~~\\max_{\\pi}V'^\\pi = c \\times \\min_{\\pi}(V^\\pi) +\\frac{d}{1- \\gamma}$    ; so the optimal policy of $V^\\pi $ is not the optimal policy of $V'^\\pi$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uCVgkDo9vTM"
      },
      "source": [
        "## Question 1.2\n",
        "\n",
        "Consider an infinite-horizon $\\gamma$-discounted MDP. We denote by $Q^*$ the $Q$-function of the optimal policy $\\pi^*$. Prove that, for any function $Q(s, a)$ (which is **not** necessarily the value function of a policy), the following inequality holds for any state $s$:\n",
        "\n",
        "$$\n",
        "V^{\\pi_Q}(s) \\geq V^*(s) - \\frac{2}{1-\\gamma}||Q^*-Q||_\\infty,\n",
        "$$\n",
        "\n",
        "where $||Q^*-Q||_\\infty = \\max_{s, a} |Q^*(s, a) - Q(s, a)|$ and $\\pi_Q(s) \\in \\arg\\max_a Q(s, a)$. Can you use this result to show that any policy $\\pi$ such that $\\pi(s) \\in \\arg\\max_a Q^*(s, a)$ is optimal?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MqGWPPD_OAI"
      },
      "source": [
        "### **Answer**\n",
        "1)$~~$We will first show the following result, $$\\forall s \\in S,~~~~ 2||Q^* - Q ||_\\infty \\geq Q^*(s, \\pi^*(s))- Q^*(s, \\pi_Q(s)) \\text{ (*)}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "To do so, we do a case disjunction,\n",
        "\n",
        "<br>\n",
        "\n",
        "Let $s \\in S$,\n",
        " <br>\n",
        "\n",
        "Suppose that $Q(s,\\pi_Q(s)) \\leq \\frac{Q^*(s, \\pi^*(s))+Q^*(s,\\pi_Q(s))}{2}$\n",
        "\n",
        "<br>\n",
        "\n",
        "then $$\\begin{align}Q^*(s,\\pi^*(s))-Q(s,\\pi_Q(s)) &\\geq Q^*(s,\\pi^*(s)) - (Q^*(s, \\pi^*(s))+Q^*(s,\\pi_Q(s)))/2 \\\\&= \\frac{Q^*(s, \\pi^*(s))-Q^*(s,\\pi_Q(s))}{2}\\end{align}$$\n",
        "\n",
        "<br>\n",
        "\n",
        " $Q^*(s,\\pi^*(s))-Q(s,\\pi_Q(s)) = \\max_{a \\in A}Q^*(s,a)-\\max_{a \\in A}Q(s,a)\\leq \\max_{a \\in A}(Q^*(s,a)-Q(s,a)) \\leq \\max_{(a,s) \\in A\\times S}(Q^*(s,a)-Q(s,a))= || Q^* - Q||_\\infty $\n",
        "\n",
        "<br>\n",
        "\n",
        "We deduce that (*) for this case.\n",
        "\n",
        "<br>\n",
        "\n",
        "Suppose that $Q(s,\\pi_Q(s)) \\geq \\frac{Q^*(s, \\pi^*(s))+Q^*(s,\\pi_Q(s))}{2}$ :\n",
        "\n",
        "$Q(s,\\pi_Q(s)) - Q^*(s,\\pi_Q(s)) \\geq \\frac{Q^*(s, \\pi^*(s))-Q^*(s,\\pi_Q(s))}{2}$\n",
        "\n",
        "<br>\n",
        "\n",
        "We still have that $Q(s,\\pi_Q(s)) - Q^*(s,\\pi_Q(s)) = |Q(s,\\pi_Q(s)) - Q^*(s,\\pi_Q(s))|\\leq || Q^* - Q||_\\infty$\n",
        "\n",
        "<br>\n",
        "\n",
        "We deduce (*) for this second case.\n",
        "\n",
        "<br>\n",
        "<b/>We have proven (*).</b>\n",
        "\n",
        "<br><br>\n",
        "2)$~~$(*) can be rewritten $$\\forall s \\in S,~~~~ 2||Q^* - Q ||_\\infty \\geq Q^*(s, \\pi^*(s))-Q^{\\pi_Q}(s, \\pi_Q(s))+Q^{\\pi_Q}(s, \\pi_Q(s))- Q^*(s, \\pi_Q(s))$$\n",
        "where $Q^{\\pi_Q(s)}$ is the action-state function induced by $\\pi_Q$ for the MDP $M$.\n",
        "\n",
        "<br>\n",
        "\n",
        "We have that $Q^*(s, \\pi^*(s))= V^*(s)$ and $-Q^{\\pi_Q}(s, \\pi_Q(s))\\geq -\\max_a Q^{\\pi_Q}(s, a)=- V^{\\pi_Q}(s) $, thus $ Q^*(s, \\pi^*(s))-Q^{\\pi_Q}(s, \\pi_Q(s))\\geq V^*(s) - V^{\\pi_Q}(s) $\n",
        "\n",
        "<br>\n",
        "\n",
        "Moreover, we have $Q^*(s, \\pi_Q(s))- Q^{\\pi_Q}(s, \\pi_Q(s))= \\gamma \\sum_{y \\in S}p(y|s,\\pi_Q(s))(V^*(y)-V^{\\pi_Q}(y)) $ (by definition of action-state value functions).\n",
        "\n",
        "<br>\n",
        " We get that $Q^*(s, \\pi_Q(s))- Q^{\\pi_Q}(s, \\pi_Q(s))\\leq \\gamma \\sum_{y \\in S}p(y|s,\\pi_Q(s)) ||V^* - V^{\\pi_Q}||_\\infty  =  \\gamma ||V^* - V^{\\pi_Q}||_\\infty$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        " $$\\forall s \\in S, ~~Q^*(s, \\pi_Q(s))- Q^{\\pi_Q}(s, \\pi_Q(s)) +2||Q^* - Q ||_\\infty \\geq Q^*(s, \\pi^*(s))-Q^{\\pi_Q}(s, \\pi_Q(s))$$\n",
        "which implies\n",
        "$$\\forall s \\in S, ~~ \\gamma ||V^* - V^{\\pi_Q}||_\\infty +2||Q^* - Q ||_\\infty \\geq V^*(s) - V^{\\pi_Q}(s) ~~~~~~\\text{(**)}$$\n",
        "which implies $$\\gamma ||V^* - V^{\\pi_Q}||_\\infty +2||Q^* - Q ||_\\infty \\geq ||V^* - V^{\\pi_Q}||_\\infty ~~~~~~\\text{(***)}$$\n",
        "\n",
        "<br>\n",
        "3)$~~$We have to show that $$\\forall s \\in S, \\forall n \\in \\mathbb{N}^*,~~ \\gamma^n ||V^* - V^{\\pi_Q}||_\\infty +2||Q^* - Q ||_\\infty \\sum_{k=0}^{n-1}\\gamma^k \\geq V^*(s) - V^{\\pi_Q}(s)$$\n",
        "<br><br>\n",
        "\n",
        "For the base case, $n=1$, it is true because it is (*).\n",
        "\n",
        "<br>\n",
        "\n",
        "For the inductive step, if it is true at $n$, as we have $$\\gamma^n ||V^* - V^{\\pi_Q}||_\\infty \\leq \\gamma^n (\\gamma ||V^* - V^{\\pi_Q}||_\\infty+2||Q^* - Q ||_\\infty) $$ we get $$\\gamma^{n+1} ||V^* - V^{\\pi_Q}||_\\infty +2||Q^* - Q ||_\\infty \\sum_{k=0}^{n}\\gamma^k \\geq V^*(s) - V^{\\pi_Q}(s)$$\n",
        "\n",
        "This completes the proof.\n",
        "\n",
        "By making $n$ tend to $\\infty$, we get $$V^*(s)- V^{\\pi_Q}(s)\\leq   \\frac{2}{1-\\gamma}||Q^*-Q||_\\infty$$\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "If we have $\\pi(s) \\in \\arg\\max_a Q^*(s, a)$, then by changing $Q$ in $Q^*$, we have the following equation : $V^{\\pi}(s) \\geq V^*(s)$.\n",
        "<br>\n",
        "On the other hand, we have $V^{\\pi}(s) \\leq V^*(s)$ from the definition of the optimal policy.\n",
        "\n",
        "<br>\n",
        "Combining these two results, we get $V^{\\pi}(s) = V^*(s)$. This means $\\pi$ is an opimal policy.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIrtb7sihYcM"
      },
      "source": [
        "## Question 1.3\n",
        "\n",
        "In this question, you will implement and compare the policy and value iteration algorithms for a finite MDP.\n",
        "\n",
        "Complete the functions `policy_evaluation`, `policy_iteration` and `value_iteration` below.\n",
        "\n",
        "\n",
        "Compare value iteration and policy iteration. Highlight pros and cons of each method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLmQtk-wt0HS"
      },
      "source": [
        "### **Answer**\n",
        "The policy iteration converges in a few steps compared to value iteration. But for policy iteration, at each step, updating $\\pi_k$ is more costly compared to value iteration.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yI0YYtMmpDQ"
      },
      "source": [
        "def policy_evaluation(P, R, policy, gamma, tol):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        P: np.array\n",
        "            transition matrix (NsxNaxNs)\n",
        "        R: np.array\n",
        "            reward matrix (NsxNa)\n",
        "        policy: np.array\n",
        "            matrix mapping states to action (Ns)\n",
        "        gamma: float\n",
        "            discount factor\n",
        "        tol: float\n",
        "            precision of the solution\n",
        "    Return:\n",
        "        value_function: np.array\n",
        "            The value function of the given policy\n",
        "    \"\"\"\n",
        "    Ns, Na = R.shape\n",
        "    # ====================================================\n",
        "\t  # YOUR IMPLEMENTATION HERE\n",
        "    #\n",
        "    value_function = np.zeros(Ns)\n",
        "    value_function_iter = np.zeros(Ns)\n",
        "    assertion = True\n",
        "\n",
        "\n",
        "    while assertion:\n",
        "      value_function = value_function_iter.copy()\n",
        "\n",
        "      for s in range(Ns):\n",
        "        value_function_iter[s]= R[s,policy[s]] + gamma*sum([P[s,policy[s],y]*value_function[y] for y in range(Ns)])\n",
        "\n",
        "\n",
        "      assertion = max(abs(value_function_iter - value_function))>tol\n",
        "\n",
        "\n",
        "    # ====================================================\n",
        "    return value_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncqbPx99ncVY"
      },
      "source": [
        "def policy_iteration(P, R, gamma, tol):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        P: np.array\n",
        "            transition matrix (NsxNaxNs)\n",
        "        R: np.array\n",
        "            reward matrix (NsxNa)\n",
        "        gamma: float\n",
        "            discount factor\n",
        "        tol: float\n",
        "            precision of the solution\n",
        "    Return:\n",
        "        policy: np.array\n",
        "            the final policy\n",
        "        V: np.array\n",
        "            the value function associated to the final policy\n",
        "    \"\"\"\n",
        "    Ns, Na = R.shape\n",
        "    V = np.zeros(Ns)\n",
        "    policy = np.ones(Ns, dtype=int)\n",
        "    # ====================================================\n",
        "\t  # YOUR IMPLEMENTATION HERE\n",
        "    #\n",
        "    policy_iter = np.zeros(Ns, dtype=int)\n",
        "    V_maximize = np.zeros((Ns,Na))\n",
        "    assertion = True\n",
        "\n",
        "    while assertion  :\n",
        "\n",
        "      V =  policy_evaluation(P, R, policy, gamma, tol)\n",
        "      for a in range(Na):\n",
        "        for s in range(Ns):\n",
        "          V_maximize[s,a] = R[s,a] + gamma * sum(P[s,a,:]*V[:])\n",
        "\n",
        "\n",
        "\n",
        "      policy_iter = policy.copy()\n",
        "      policy = np.argmax(V_maximize, axis = 1)\n",
        "\n",
        "      for s in range(Ns):\n",
        "        assertion = not( (policy - policy_iter)[s] == 0)\n",
        "\n",
        "        if assertion :\n",
        "          break\n",
        "\n",
        "\n",
        "\n",
        "    # ====================================================\n",
        "    return policy, V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jB7dfA5nRCZ"
      },
      "source": [
        "def value_iteration(P, R, gamma, tol):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        P: np.array\n",
        "            transition matrix (NsxNaxNs)\n",
        "        R: np.array\n",
        "            reward matrix (NsxNa)\n",
        "        gamma: float\n",
        "            discount factor\n",
        "        tol: float\n",
        "            precision of the solution\n",
        "    Return:\n",
        "        Q: final Q-function (at iteration n)\n",
        "        greedy_policy: greedy policy wrt Qn\n",
        "        Qfs: all Q-functions generated by the algorithm (for visualization)\n",
        "    \"\"\"\n",
        "    Ns, Na = R.shape\n",
        "    Q = np.zeros((Ns, Na))\n",
        "    Qfs = [Q]\n",
        "    assertion = True\n",
        "    # ====================================================\n",
        "\t  # YOUR IMPLEMENTATION HERE\n",
        "    #\n",
        "    while assertion :\n",
        "\n",
        "\n",
        "      Q_max = np.max(Q,axis= 1)\n",
        "\n",
        "      Q = R + gamma*np.array([[sum(P[s,a,:]*Q_max[:])for a in range(Na) ] for s in range(Ns)])\n",
        "      greedy_policy = np.argmax(Q, axis=1)\n",
        "\n",
        "      Qfs.append(Q)\n",
        "      assertion = np.max(abs(Qfs[-1]-Qfs[-2])) > tol\n",
        "\n",
        "\n",
        "    # ====================================================\n",
        "    return Q, greedy_policy, Qfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fi0IzZJp74Z"
      },
      "source": [
        "### Testing your code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7JKrc1oqFI2"
      },
      "source": [
        "# Parameters\n",
        "tol = 1e-5\n",
        "gamma = 0.99\n",
        "\n",
        "# Environment\n",
        "env = get_env()\n",
        "Ns, Na = env.R.shape\n",
        "\n",
        "# run value iteration to obtain Q-values\n",
        "VI_Q, VI_greedypol, all_qfunctions = value_iteration(env.P, env.R, gamma, tol)\n",
        "\n",
        "# render the policy\n",
        "print(\"[VI]Greedy policy: \")\n",
        "render_policy(env, VI_greedypol)\n",
        "\n",
        "# compute the value function of the greedy policy using matrix inversion\n",
        "# ====================================================\n",
        "# YOUR IMPLEMENTATION HERE\n",
        "#greedy_V = policy_evaluation(env.P, env.R, VI_greedypol, gamma, tol)\n",
        "greedy_V = np.linalg.inv(np.eye(Ns)- gamma* np.array([[env.P[s,VI_greedypol[s],y] for y in range(Ns)]for s in range(Ns)])) @ np.array([env.R[s,VI_greedypol[s]] for s in range(Ns)])\n",
        "# compute value function of the greedy policy\n",
        "#\n",
        "\n",
        "# ====================================================\n",
        "\n",
        "# show the error between the computed V-functions and the final V-function\n",
        "# (that should be the optimal one, if correctly implemented)\n",
        "# as a function of time\n",
        "final_V = all_qfunctions[-1].max(axis=1)\n",
        "norms = [ np.linalg.norm(q.max(axis=1) - final_V) for q in all_qfunctions]\n",
        "plt.plot(norms)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.title(\"Value iteration: convergence\")\n",
        "\n",
        "#### POLICY ITERATION ####\n",
        "PI_policy, PI_V = policy_iteration(env.P, env.R, gamma, tol)\n",
        "print(\"\\n[PI]final policy: \")\n",
        "render_policy(env, PI_policy)\n",
        "\n",
        "## Uncomment below to check that everything is correct\n",
        "assert np.allclose(PI_policy, VI_greedypol)\n",
        "#     \"You should check the code, the greedy policy computed by VI is not equal to the solution of PI\"\n",
        "assert np.allclose(PI_V, greedy_V, atol=0.001) #precision here had to be put to 0.001\n",
        "#     \"Since the policies are equal, even the value function should be\"\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V1QdoH-xFX0"
      },
      "source": [
        "# Part 2 - Tabular RL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf51VhoPxbV4"
      },
      "source": [
        "## Question 2.1\n",
        "\n",
        "The code below collects two datasets of transitions (containing states, actions, rewards and next states) for a discrete MDP.\n",
        "\n",
        "For each of the datasets:\n",
        "\n",
        "1. Estimate the transitions and rewards, $\\hat{P}$ and $\\hat{R}$.\n",
        "2. Compute the optimal value function and the optimal policy with respect to the estimated MDP (defined by $\\hat{P}$ and $\\hat{R}$), which we denote by $\\hat{\\pi}$ and $\\hat{V}$.\n",
        "3. Numerically compare the performance of $\\hat{\\pi}$ and $\\pi^\\star$ (the true optimal policy), and the error between $\\hat{V}$ and $V^*$ (the true optimal value function).\n",
        "\n",
        "Which of the two data collection methods do you think is better? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWSyewG2EZpJ"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "The dataset 2 is the best, for dataset 1 it happens we have a very large error.\n",
        "\n",
        "<br>\n",
        "\n",
        "We learn something only when we reach the terminal state. When we sample randomly from all possible pairs $(s,a)$, we get the terminal state with probability $1/S=1/31$. For dataset 2, we must have at least 3 times this number, ie 100 samples work.\n",
        "\n",
        "<br>\n",
        "\n",
        "When we sample states for a given trajectory as for dataset 1, the terminal state is very less likely to be visited. When it is not visited, we don't update $R(s,a)$, then our computations lead to $Q(s,a)=0$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lNPhB28EcGd"
      },
      "source": [
        "def get_random_policy_dataset(env, n_samples):\n",
        "  \"\"\"Get a dataset following a random policy to collect data.\"\"\"\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  next_states = []\n",
        "\n",
        "  state = env.reset()\n",
        "  for _ in range(n_samples):\n",
        "    action = env.action_space.sample()\n",
        "    next_state, reward, is_terminal, info = env.step(action)\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    next_states.append(next_state)\n",
        "    # update state\n",
        "    state = next_state\n",
        "    if is_terminal:\n",
        "      state = env.reset()\n",
        "\n",
        "  dataset = (states, actions, rewards, next_states)\n",
        "  return dataset\n",
        "\n",
        "def get_uniform_dataset(env, n_samples):\n",
        "  \"\"\"Get a dataset by uniformly sampling states and actions.\"\"\"\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  next_states = []\n",
        "  for _ in range(n_samples):\n",
        "    state = env.observation_space.sample()\n",
        "    action = env.action_space.sample()\n",
        "    next_state, reward, is_terminal, info = env.sample(state, action)\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    next_states.append(next_state)\n",
        "\n",
        "  dataset = (states, actions, rewards, next_states)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Collect two different datasets\n",
        "num_samples = 500 #2000\n",
        "env = get_env()\n",
        "dataset_1 = get_random_policy_dataset(env, num_samples)\n",
        "dataset_2 = get_uniform_dataset(env, num_samples)\n",
        "\n",
        "\n",
        "# Item 3: Estimate the MDP with the two datasets; compare the optimal value\n",
        "# functions in the true and in the estimated MDPs\n",
        "Ns, Na = env.R.shape\n",
        "def Compute_Reward_and_Probability(dataset):\n",
        "  \"It returns the estimated probabilities P(Ns,Na,Ns) and estimated rewards R (Ns,Na)  \"\n",
        "  N_a_s = np.zeros((Ns,Na))\n",
        "  R = np.zeros((Ns, Na))\n",
        "  P = np.zeros((Ns,Na,Ns))\n",
        "\n",
        "  states, actions, rewards, next_states = dataset\n",
        "  for count in range(num_samples):\n",
        "    s,a,y = states[count],actions[count], next_states[count]\n",
        "\n",
        "    for x in range(Ns): #update values of P\n",
        "      if x ==y:\n",
        "        P[s,a,y] = (P[s,a,y]*N_a_s[s,a]+1)/(N_a_s[s,a]+1)\n",
        "\n",
        "      else:\n",
        "        P[s,a,x] = (P[s,a,x]*N_a_s[s,a])/(N_a_s[s,a]+1)\n",
        "\n",
        "    R[s,a] = (R[s,a]*N_a_s[s,a]+rewards[count])/(N_a_s[s,a]+1) #update values of R\n",
        "\n",
        "    N_a_s[s,a]+=1 #update the number of times action a was taken when in state s\n",
        "\n",
        "  return P, R\n",
        "\n",
        "P_1, R_1 = Compute_Reward_and_Probability(dataset_1) #this returns estimated probabilities and rewards for dataset 1\n",
        "P_2, R_2 = Compute_Reward_and_Probability(dataset_2) #same for dataset 2\n",
        "\n",
        "Q_1, pi_1 = value_iteration(P_1, R_1, gamma, tol)[:2] #we compute hat{V} and hat{pi} with environnement estimated with dataset1\n",
        "Q_2, pi_2 = value_iteration(P_2, R_2, gamma, tol)[:2]\n",
        "\n",
        "Q_star, pi_star = value_iteration(env.P, env.R, gamma, tol)[:2]\n",
        "\n",
        "V_1 = np.max(Q_1, axis=1)\n",
        "V_2 = np.max(Q_2, axis=1)\n",
        "V_star = np.max(Q_star, axis=1)\n",
        "\n",
        "print(\"The error between V* and hat{v} are: \\n *\", max(abs(V_1 - V_star)), \" for the first dataset; \\n *\",max(abs(V_2 - V_star)), \" for the second dataset; \\n\")\n",
        "\n",
        "# ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKINsa_yGLGL"
      },
      "source": [
        "## Question 2.2\n",
        "\n",
        "Suppose that $\\hat{P}$ and $\\hat{R}$ are estimated from a dataset of exactly $N$ i.i.d. samples from **each** state-action pair. This means that, for each $(s,a)$, we have $N$ samples $\\{(s_1',r_1, \\dots, s_N', r_N\\}$, where $s_i' \\sim P(\\cdot | s,a)$ and $r_i \\sim R(s,a)$ for $i=1,\\dots,N$, and\n",
        "$$ \\hat{P}(s'|s,a) = \\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}(s_i' = s'), $$\n",
        "$$ \\hat{R}(s,a) = \\frac{1}{N}\\sum_{i=1}^N r_i.$$\n",
        "Suppose that $R$ is a distribution with support in $[0,1]$. Let $\\hat{V}$ be the optimal value function computed in the empirical MDP (i.e., the one with transitions $\\hat{P}$ and rewards $\\hat{R}$). For any $\\delta\\in(0,1)$, derive an upper bound to the error\n",
        "\n",
        "$$ \\| \\hat{V} - V^* \\|_\\infty $$\n",
        "\n",
        "which holds with probability at least $1-\\delta$.\n",
        "\n",
        "**Note** Your bound should only depend on deterministic quantities like $N$, $\\gamma$, $\\delta$, $S$, $A$. It should *not* dependent on the actual random samples.\n",
        "\n",
        "**Hint** The following two inequalities may be helpful.\n",
        "\n",
        "1. **A (simplified) lemma**. For any state $\\bar{s}$,\n",
        "\n",
        "$$ |\\hat{V}(\\bar{s}) - V^*(\\bar{s})| \\leq \\frac{1}{1-\\gamma}\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right|$$\n",
        "\n",
        "2. **Hoeffding's inequality**. Let $X_1, \\dots X_N$ be $N$ i.i.d. random variables bounded in the interval $[0,b]$ for some $b>0$. Let $\\bar{X} = \\frac{1}{N}\\sum_{i=1}^N X_i$ be the empirical mean. Then, for any $\\epsilon > 0$,\n",
        "\n",
        "$$ \\mathbb{P}(|\\bar{X} - \\mathbb{E}[\\bar{X}]| > \\epsilon) \\leq 2e^{-\\frac{2N\\epsilon^2}{b^2}}.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKmdulLaMoiN"
      },
      "source": [
        "### **Answer**\n",
        "1)\n",
        "We have $||\\hat{V}-V^*||_\\infty \\leq \\frac{1}{1-\\gamma}\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right| :=\\max_{s,a}M(s,a)$\n",
        "\n",
        "<br>\n",
        "\n",
        "We have $$\\begin{align}\\mathbb{P}(||\\hat{V}-V^*||_\\infty \\leq \\epsilon)&\\geq \\mathbb{P}(M \\leq \\epsilon)& \\text{ inclusion of events}\n",
        "\\\\&= \\prod_{s,a}\\mathbb{P}(M(s,a) \\leq \\epsilon) \\end{align} $$\n",
        "\n",
        "To ensure that the maximum $M$ is smaller than a value, you have to ensure that all components $M(s,a)$ are smaller than the same value.\n",
        "\n",
        "<br>\n",
        "\n",
        "We notice that $\\mathbb{P}(M(s,a) \\leq \\epsilon)= 1 - \\mathbb{P}(M(s,a) \\geq \\epsilon)$.  \n",
        "\n",
        "<br><br>\n",
        "\n",
        "2) We introduce $X_i=\\frac{1}{1-\\gamma}(r_i + \\gamma \\sum_{s'} 1(s_i' = s')V^*(s'))$ for $i \\in \\{1,\\dots ,N\\}$. We have the two following equalities.\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\\begin{align}(1-\\gamma)\\bar{X}=\\frac{1}{N} \\sum_i r_i + \\gamma \\sum_{s'}(\\frac{1}{N}\\sum_i  1(s_i' = s'))V^*(s') = \\hat{R}(s,a)+\\gamma \\sum_{s'}\\hat{P}(s'|s,a)V^*(s') &~~~~~~~~~~~~&\\text{(1)}\\\\ &&\\\\(1-\\gamma)\\mathbb{E}[\\bar{X}] = R(s,a) + \\gamma \\sum_{s'}P(s'|s,a)V^*(s')&& \\text{(2)} \\end{align}$$.\n",
        "\n",
        "<br>\n",
        "\n",
        "Combining (1) and (2),\n",
        "we get $|\\bar{X} - \\mathbb{E}[\\bar{X}]|= M(s,a)$.\n",
        "\n",
        "<br>\n",
        "\n",
        "3)We now have to show that $X_i$ is in $[0,\\frac{1}{(1 - \\gamma)^2}]$\n",
        "\n",
        "<br>\n",
        "We have, $V^*(s) = \\mathbb{E}[\\sum_{k=0}^\\infty \\gamma^k r(s,a)|s_0=s, \\pi^*]\\leq \\mathbb{E}[ \\sum_{k=0}^\\infty \\gamma^k] = \\frac{1}{1-\\gamma}$\n",
        "\n",
        "We deduce $0 < V^*(s)< \\frac{1}{1-\\gamma}$ and by definition, $0 < r_i < 1 $.\n",
        "\n",
        "$X_i>0$ is obvious. $X_i =\\frac{1}{1-\\gamma}( r_i + \\gamma \\sum_{s'}1(s_i=s')V(s')) \\leq \\frac{1}{1-\\gamma}(1+  \\gamma \\frac{1}{1 - \\gamma})= \\frac{1}{(1-\\gamma)^2}:=b$\n",
        "\n",
        "<br>\n",
        "\n",
        "4) $X_1,...,X_n$ are independent and identically distributed as the samples are iid.\n",
        "We use Hoeffding's inequality:\n",
        "$$ \\mathbb{P}(M(s,a) > \\epsilon) \\leq 2e^{-\\frac{2N\\epsilon^2}{b^2}}.$$\n",
        "\n",
        "<br>\n",
        "We now can bound from below  $\\prod_{s,a}\\mathbb{P}(M(s,a) \\leq \\epsilon)$.\n",
        "\n",
        "$\\prod_{s,a}\\mathbb{P}(M(s,a) \\leq \\epsilon)=\\prod_{s,a}(1-\\mathbb{P}(M(s,a) \\geq \\epsilon))\\geq \\prod_{s,a}(1- 2e^{-\\frac{2N\\epsilon^2}{b^2}})=1-\\delta$.\n",
        "\n",
        "<br>\n",
        "\n",
        "<b>  \n",
        "\n",
        "\n",
        "We get the following bound :$$\\mathbb{P}\\left(||\\hat{V}-V^*||_\\infty \\leq \\frac{1}{\\sqrt{2N}(1-\\gamma)^2}\\sqrt{\\ln{\\left(\\frac{2}{1-(1-\\delta)^{1/AS}}\\right)}} \\right) \\geq 1- \\delta$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpqwCBG2MwxO"
      },
      "source": [
        "## Question 2.3\n",
        "\n",
        "Suppose once again that we are given a dataset of $N$ samples in the form of tuples $(s_i,a_i,s_i',r_i)$. We know that each tuple contains a valid transition from the true MDP, i.e., $s_i' \\sim P(\\cdot | s_i, a_i)$ and $r_i \\sim R(s_i,a_i)$, while the state-action pairs $(s_i,a_i)$ from which the transition started can be arbitrary.\n",
        "\n",
        "Suppose we want to apply Q-learning to this MDP. Can you think of a way to leverage this offline data to improve the sample-efficiency of the algorithm? What if we were using SARSA instead?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbYTKetHOYU_"
      },
      "source": [
        "### **Answer**\n",
        "The Sequential updates produce correlated samples. A good way to remedy that is to use experience replay.\n",
        "\n",
        "Instead of sampling uniformely over the state - action to get $(s,a)$, you input a dataset. At each step, the sample $(a,s)$ is drawn from the dataset, the agent evolves to a new state $s'$.\n",
        "\n",
        "This reinforces that the most recurring states for the MDP will be appearing many more times during the sampling. While the least recurring states for the MDP will be almost never sampled.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "542QxKsSOs21"
      },
      "source": [
        "# Part 3 - RL with Function Approximation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiGZBiJ4PiIE"
      },
      "source": [
        "## Question 3.1\n",
        "\n",
        "Given a datset $(s_i, a_i, r_i, s_i')$ of (states, actions, rewards, next states), the Fitted Q-Iteration (FQI) algorithm proceeds as follows:\n",
        "\n",
        "\n",
        "* We start from a $Q$ function $Q_0 \\in \\mathcal{F}$, where $\\mathcal{F}$ is a function space;\n",
        "* At every iteration $k$, we compute $Q_{k+1}$ as:\n",
        "\n",
        "$$\n",
        "Q_{k+1}\\in\\arg\\min_{f\\in\\mathcal{F}} \\frac{1}{2}\\sum_{i=1}^N\n",
        "\\left(\n",
        "  f(s_i, a_i) - y_i^k\n",
        "\\right)^2 + \\lambda \\Omega(f)\n",
        "$$\n",
        "where $y_i^k = r_i + \\gamma \\max_{a'}Q_k(s_i', a')$, $\\Omega(f)$ is a regularization term and $\\lambda > 0$ is the regularization coefficient.\n",
        "\n",
        "\n",
        "Consider FQI with *linear* function approximation. That is, for a given feature map $\\phi : S \\rightarrow \\mathbb{R}^d$, we consider a parametric family of $Q$ functions $Q_\\theta(s,a) = \\phi(s)^T\\theta_a$ for $\\theta_a\\in\\mathbb{R}^d$. Suppose we are applying FQI on a given dataset of $N$ tuples of the form $(s_i, a_i, r_i, s_i')$ and we are at the $k$-th iteration. Let $\\theta_k \\in\\mathbb{R}^{d \\times A}$ be our current parameter. Derive the *closed-form* update to find $\\theta_{k+1}$, using $\\frac{1}{2}\\sum_a ||\\theta_a||_2^2$ as regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jx7aE41DkEM"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "$h:\\theta =( \\theta_1,\\dots,\\theta_A )\\mapsto \\frac{1}{2}\\left[\\sum_{i=1}^N\n",
        "\\left(\n",
        "   \\phi(s_i)^T\\theta_{ a_i} - y_i^k\n",
        "\\right)^2 + \\lambda \\sum_a ||\\theta_a||_2^2\\right] $ is a convex and coercitive function. Then it admits a unique minimalizer on $\\mathbb{R}^{dA}$.\n",
        "\n",
        "<br>\n",
        "We will compute the gradient of h with respect to $\\theta$ and find the minimalizer $\\theta^*$ for which $\\nabla_\\theta h(\\theta^*)=\\vec{0}$.\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\\nabla_{\\theta_j} h = \\sum_{i=1}^N\n",
        "\\left(\n",
        "   \\phi(s_i)^T\\theta_{ a_i} - y_i^k\n",
        "\\right) \\delta(a_i=j) \\phi(s_i) + \\lambda \\theta_j$$\n",
        "\n",
        "We derive $\\theta^*$:\n",
        "$$\\begin{align} \\nabla_{\\theta_j} h =& 0\\\\ ⇔\n",
        "(\\lambda \\mathbb{I}_d + \\sum_{i=1}^N \\delta(a_i = j)\\phi(s_i)\\phi(s_i)^T)\\theta_j^*= &\\sum_{i=1}^N y_i^k \\delta(a_i=j)\\phi(s_i)  \\end{align}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\phi(s_i)\\phi(s_i)^T$ is a matrix with eigenvalues $||\\phi(s_i)||_2^2$ and 0 so all its eigenvalues are positive.\n",
        "\n",
        "<br>\n",
        "\n",
        "A sum of positive-eingenvalue operators are also a positive-eigenvalue operator. This is because for positive-eigenvalue operators $A$, we have $\\forall X \\in \\mathbb{R}^d$, $X^TAX \\geq 0$. When we sum, we keep the bound from below by 0.\n",
        "\n",
        "<br> Then $\\lambda \\mathbb{I}_d + \\sum_{i=1}^N \\delta(a_i = j)\\phi(s_i)\\phi(s_i)^T$ is invertible.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "The actuation of $\\theta$ is:\n",
        "$$\\theta_{k+1} = (\\theta^*_1,\\dots, \\theta^*_A)$$  where $\\theta_j$ is defined as:\n",
        "$$\\theta_j^*=\n",
        "\\left(\\lambda \\mathbb{I}_d + \\sum_{i=1}^N \\delta(a_i = j)\\phi(s_i)\\phi(s_i)^T \\right)^{-1} \\sum_{i=1}^N y_i^k \\delta(a_i=j)\\phi(s_i) $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewHzjm7MVGBg"
      },
      "source": [
        "## Question 3.2\n",
        "\n",
        "The code below creates a larger gridworld (with more states than the one used in the previous questions), and defines a feature map. Implement linear FQI to this environment (in the function `linear_fqi()` below), and compare the approximated $Q$ function to the optimal $Q$ function computed with value iteration.\n",
        "\n",
        "Can you improve the feature map in order to reduce the approximation error?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu4g-HSnEcBs"
      },
      "source": [
        "### **Answer**\n",
        "\n",
        "[explanation about how you tried to reduce the approximation error + FQI implementation below]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZovF3VXOVfCs"
      },
      "source": [
        "def get_large_gridworld():\n",
        "  \"\"\"Creates an instance of a grid-world MDP with more states.\"\"\"\n",
        "  walls = [(ii, 10) for ii in range(15) if (ii != 7 and ii != 8)]\n",
        "  env = GridWorld(\n",
        "      nrows=15,\n",
        "      ncols=15,\n",
        "      reward_at = {(14, 14):1.0},\n",
        "      walls=tuple(walls),\n",
        "      success_probability=0.9,\n",
        "      terminal_states=((14, 14),)\n",
        "  )\n",
        "  return env\n",
        "\n",
        "\n",
        "class GridWorldFeatureMap:\n",
        "  \"\"\"Create features for state-action pairs\n",
        "\n",
        "  Args:\n",
        "    dim: int\n",
        "      Feature dimension\n",
        "    sigma: float\n",
        "      RBF kernel bandwidth\n",
        "  \"\"\"\n",
        "  def __init__(self, env, dim=15, sigma=0.25):\n",
        "    self.index2coord = env.index2coord\n",
        "    self.n_states = env.Ns\n",
        "    self.n_actions = env.Na\n",
        "    self.dim = dim\n",
        "    self.sigma = sigma\n",
        "\n",
        "    n_rows = env.nrows\n",
        "    n_cols = env.ncols\n",
        "\n",
        "    # build similarity matrix\n",
        "    sim_matrix = np.zeros((self.n_states, self.n_states))\n",
        "    for ii in range(self.n_states):\n",
        "        row_ii, col_ii = self.index2coord[ii]\n",
        "        x_ii = row_ii / n_rows\n",
        "        y_ii = col_ii / n_cols\n",
        "        for jj in range(self.n_states):\n",
        "            row_jj, col_jj = self.index2coord[jj]\n",
        "            x_jj = row_jj / n_rows\n",
        "            y_jj = col_jj / n_cols\n",
        "            dist = np.sqrt((x_jj - x_ii) ** 2.0 + (y_jj - y_ii) ** 2.0)\n",
        "            sim_matrix[ii, jj] = np.exp(-(dist / sigma) ** 2.0)\n",
        "\n",
        "    # factorize similarity matrix to obtain features\n",
        "    uu, ss, vh = np.linalg.svd(sim_matrix, hermitian=True)\n",
        "    self.feats = vh[:dim, :]\n",
        "\n",
        "  def map(self, observation):\n",
        "    feat = self.feats[:, observation].copy()\n",
        "    return feat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InCfu7F9-TbS"
      },
      "source": [
        "env = get_large_gridworld()\n",
        "feat_map = GridWorldFeatureMap(env)\n",
        "\n",
        "# Visualize large gridworld\n",
        "render_policy(env)\n",
        "\n",
        "# The features have dimension (feature_dim).\n",
        "feature_example = feat_map.map(1) # feature representation of s=1\n",
        "print(feature_example)\n",
        "\n",
        "# Initial vector theta representing the Q function\n",
        "theta = np.zeros((feat_map.dim, env.action_space.n))\n",
        "print(theta.shape)\n",
        "print(feature_example @ theta) # approximation of Q(s=1, a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p21KMmruugO1"
      },
      "source": [
        "def linear_fqi(env, feat_map, num_iterations, gamma,lambd):\n",
        "  \"\"\"\n",
        "  # Linear FQI implementation\n",
        "  # TO BE COMPLETED\n",
        "  \"\"\"\n",
        "  N = 5000 #number of samples\n",
        "  # get a dataset\n",
        "  dataset = get_uniform_dataset(env, n_samples=N) #we chose the number of samples\n",
        "  # OR dataset = get_random_policy_dataset(env, n_samples=...)\n",
        "\n",
        "  (states, actions, rewards, next_states) = dataset\n",
        "\n",
        "  theta = np.zeros((feat_map.dim, env.Na))\n",
        "  Y = np.zeros(N)\n",
        "  phi = feat_map.feats\n",
        "\n",
        "  Z = np.zeros((feat_map.dim, env.Na))\n",
        "  A = np.zeros((feat_map.dim, feat_map.dim, env.Na))\n",
        "  #we compute only once the invert of the big matrix (lamba Id + sum phi(si)Tphi(si)), we call it A\n",
        "\n",
        "  for i in range(N):\n",
        "    A[:,:,actions[i]] = A[:,:,actions[i]] + np.outer(phi[:,states[i]],phi[:,states[i]])\n",
        "\n",
        "  for action in range(Na):\n",
        "    A[:, :, action] = A[:, :, action] + lambd * np.eye(feat_map.dim)\n",
        "    A[:, :, action] = np.linalg.inv(A[:,:,action])\n",
        "\n",
        "\n",
        "  for it in range(num_iterations):\n",
        "    for i in range(N):\n",
        "      Y[i] = rewards[i]+ gamma* np.max( phi[:,next_states[i]]@theta)\n",
        "\n",
        "    for i in range(N):\n",
        "      Z[:,actions[i]]+= Y[i]*phi[:,states[i]] #we computed sum yi^k phi(s_i)\n",
        "\n",
        "\n",
        "    for action in range(Na):\n",
        "      theta[:,action] = A[:,:,action] @ Z[:,action] #we update theta\n",
        "\n",
        "\n",
        "\n",
        "  return theta\n",
        "\n",
        "# ----------------------------\n",
        "# Environment and feature map\n",
        "# ----------------------------\n",
        "gamma = 0.95\n",
        "lambd = 0.1\n",
        "env = get_large_gridworld()\n",
        "# you can change the parameters of the feature map, and even try other maps!\n",
        "feat_map = GridWorldFeatureMap(env, dim=150, sigma=0.5)\n",
        "\n",
        "# -------\n",
        "# Run FQI\n",
        "# -------\n",
        "theta = linear_fqi(env, feat_map,200, gamma,lambd )\n",
        "\n",
        "# Compute and run greedy policy\n",
        "Q_fqi = np.zeros((env.Ns, env.Na))\n",
        "for ss in range(env.Ns):\n",
        "  state_feat = feat_map.map(ss)\n",
        "  Q_fqi[ss, :] = state_feat @ theta\n",
        "\n",
        "V_fqi = Q_fqi.max(axis=1)\n",
        "policy_fqi = Q_fqi.argmax(axis=1)\n",
        "render_policy(env, policy_fqi, horizon=100)\n",
        "\n",
        "# Visualize the approximate value function in the gridworld.\n",
        "img = env.get_layout_img(V_fqi)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi = feat_map.feats\n",
        "np.outer(phi[:,2],phi[:,2])[5,6]==phi[5,2]*phi[6,2]"
      ],
      "metadata": {
        "id": "dYIHEgAiSVvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(phi[:,2]@theta,axis=1)"
      ],
      "metadata": {
        "id": "pou0MbhDUEqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##It is what happens with value iteration\n",
        "\n",
        "# run value iteration to obtain Q-values\n",
        "VI_Q, VI_greedypol, all_qfunctions = value_iteration(env.P, env.R, gamma, tol)\n",
        "\n",
        "# render the policy\n",
        "print(\"[VI]Greedy policy: \")\n",
        "render_policy(env, VI_greedypol)\n",
        "\n",
        "VI_V = np.max(VI_Q, axis=1)\n",
        "\n",
        "img = env.get_layout_img(np.array(VI_V))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L3quhcgU0GP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! jupyter nbconvert RL-MVA_2022-Homework_1_MarceauPAILHAS.ipnb --to pdf"
      ],
      "metadata": {
        "id": "s-jT8Cisd6VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G0bifq4ffdHe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}