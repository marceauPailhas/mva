{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0726DI1W8RJ"
   },
   "source": [
    "# Project part 2: beat flappy bird\n",
    "\n",
    "You may be familiar with the game [flappy bird](https://flappybird.io/). It is very simple: a bird moves at constant speed on the x axis and, to direct him, you can either push it up or let it fall at each step. The goal of the game is to go as far as possible.\n",
    "\n",
    "Your goal for this project is as follow: design and train an agent which does the best possible score at flappy bird !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yiSc6BlkByJV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/root/.ssh/id_ed25519'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m mkdir -p /root/.ssh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Write the key\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/.ssh/id_ed25519\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     19\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(GITHUB_PRIVATE_KEY)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Add github.com to our known hosts\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.ssh/id_ed25519'"
     ]
    }
   ],
   "source": [
    "#@title Installations  { form-width: \"30%\" }\n",
    "\n",
    "# This is just for the purpose of this colab. Please do not share a ssh\n",
    "# private key in real life, it is a really unsafe practice.\n",
    "GITHUB_PRIVATE_KEY = ",
    "-----END OPENSSH PRIVATE KEY-----\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist.\n",
    "! mkdir -p /root/.ssh\n",
    "# Write the key\n",
    "with open(\"/root/.ssh/id_ed25519\", \"w\") as f:\n",
    "    f.write(GITHUB_PRIVATE_KEY)\n",
    "# Add github.com to our known hosts\n",
    "! ssh-keyscan -t ed25519 github.com >> ~/.ssh/known_hosts\n",
    "# Restrict the key permissions, or else SSH will complain.\n",
    "! chmod go-rwx /root/.ssh/id_ed25519\n",
    "\n",
    "# Clone and install the RL Games repository\n",
    "! if [ -d \"rl_games\" ]; then echo \"rl_games directory exists.\"; else git clone git@github.com:Molugan/rl_games.git; fi\n",
    "! cd rl_games ; git pull;  pip install .\n",
    "\n",
    "# Other dependencies\n",
    "# If you just want to play your environment and does not intend to use either\n",
    "# jax or haiku you can comment this part.\n",
    "!pip install dm-acme[jax]\n",
    "!pip install dm-acme[tf]\n",
    "!pip install dm-haiku\n",
    "!pip install chex\n",
    "!pip install optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7qrEETnyOHL"
   },
   "source": [
    "## What you are expected to do:\n",
    "\n",
    "First off, constitute groups of 5 or less and fill in this [sheet](https://docs.google.com/spreadsheets/d/16TqSBGN33izSbom9-Vk2KYAxeYpEnz79-ylCHrTQiEA/edit#gid=0).\n",
    "You are then asked to:\n",
    "- Implement an agent to reach a score as high as you can on the environment within 2h of GPU compute. The code should be well designed and commented. You are not required to use reinforcement learning, but can if you find it useful. You can take inspiration from both the practicals and any online codebase that you may find useful, provided you reference it. However any suspicion of plagiarism on another team will results in grades being divided by two for both teams. The states of the environment are purposefully obfuscated, it is your job to find a representation that will be easily ingestible by whatever method you are going to be using. Two notes:\n",
    "  - Since GPU access is limited on Colab, you may want to experiment with CPUs and only use GPUs for your final run. Depending on the kind of algorithms you implement there is not necessarily going to be a huge difference.\n",
    "  - It is obviously forbidden to load external weights, that could be used to checkpoint your training.\n",
    "- Write a report (2-4 pages) explaining the approach you took in details, the hyperparameter searches you performed, and the final results you obtained.\n",
    "\n",
    "## Deadline\n",
    "You should complete this project and send us our results by April 7th 11:59pm, you will get a penalty of one point by day of delay.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "We are going to run your notebook on a Colab GPU instance for one hour and we will consider the performance of your model after that time.\n",
    "\n",
    "Grade decomposition:\n",
    "\n",
    "- Report 4pts:\n",
    "  - Method description.\n",
    "  - Hyperparameter choice explanation.\n",
    "  - Results presentation.\n",
    "- Code 4pts:\n",
    "  - Does the method described in the report match the method implemented?\n",
    "  - Is the code readable?\n",
    "  - Is the code well presented and documented?\n",
    "- Performance 5 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztgSWy0fYvY5"
   },
   "source": [
    "## The environment\n",
    "\n",
    "We will use the Flappy Bird environment defined in the deep_rl package. Let's have a closer look at it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMk5FLsimhhx"
   },
   "outputs": [],
   "source": [
    "from deep_rl.environments.flappy_bird import FlappyBird\n",
    "\n",
    "env = FlappyBird(\n",
    "        gravity=0.05,\n",
    "        force_push=0.1,\n",
    "        vx=0.05,\n",
    "        prob_new_bar=1,\n",
    "        invictus_mode=False,\n",
    "        max_height_bar=0.5,\n",
    "    )\n",
    "\n",
    "print(env.help)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOFXHhUYmhVL"
   },
   "source": [
    "For example let's interact with it a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUEklG8l726x"
   },
   "outputs": [],
   "source": [
    "rows, cols = env.min_res\n",
    "print(f\"We should use at least {rows} rows and {cols} when rendering the environment\")\n",
    "\n",
    "obs_reset = env.reset()\n",
    "print(\"First observation when reseting the environment:\")\n",
    "print(obs_reset)\n",
    "print()\n",
    "\n",
    "print(\"Now, let's perform a few steps\\n\")\n",
    "\n",
    "print(\"Step 1: we let the bird fall\")\n",
    "obs, reward, done = env.step(0)\n",
    "print(f\"Observation: {obs}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Game over: {done}\")\n",
    "print()\n",
    "\n",
    "print(\"Step 2: we push the bird up\")\n",
    "obs, reward, done = env.step(1)\n",
    "print(f\"Observation: {obs}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Game over: {done}\")\n",
    "print()\n",
    "\n",
    "print(\"Step 3: we push the bird up again\")\n",
    "obs, reward, done = env.step(1)\n",
    "print(f\"Observation: {obs}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Game over: {done}\")\n",
    "print()\n",
    "\n",
    "print(\"Step 4: we push the bird up again\")\n",
    "obs, reward, done = env.step(1)\n",
    "print(f\"Observation: {obs}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Game over: {done}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOVy4waHAivf"
   },
   "source": [
    "To simplify typing a bit, the deep_rl package implements a new type `FlappyObs` which corresponds to a state of the flappy bird environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwQsyfBBAxiA"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "BarObs = Tuple[float, float, float, bool]\n",
    "BirdObs = Tuple[float, float, float]\n",
    "FlappyObs = Tuple[BirdObs, List[BarObs]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UI7HQF1zhNcW"
   },
   "source": [
    "## Baseline\n",
    "\n",
    "We provide you with a simple baseline: the `StableAgent` which does nothing more than keeping the bird stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0xIH_5DB4f4"
   },
   "outputs": [],
   "source": [
    "from deep_rl.environments.flappy_bird import FlappyObs\n",
    "\n",
    "class StableAgent:\n",
    "  \"\"\"An agent which just keeps the bird stable.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               target_y : float = 0.5):\n",
    "    self._target_y = target_y\n",
    "\n",
    "  def sample_action(self,\n",
    "                    observation: FlappyObs,\n",
    "                    evaluation: bool,\n",
    "                    ) -> int:\n",
    "    _, y_bird, v_y_bird = observation[0]\n",
    "\n",
    "    if y_bird <= self._target_y and v_y_bird <= 0:\n",
    "      return 1\n",
    "    else:\n",
    "      return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnX6ij-gCCIN"
   },
   "source": [
    "Let's see how a single runs works in practice with this agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7EHsNT6CG0e"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from deep_rl.terminal_renderer import BashRenderer\n",
    "from deep_rl.episode_runner import run_episode\n",
    "from deep_rl.project_values import PROJECT_FLAPPY_BIRD_ENV\n",
    "\n",
    "# We are going to render the environment !\n",
    "ROWS = 30\n",
    "COLS = 60\n",
    "# Because ipython sucks, I have not found a cleaner option to add\n",
    "# the refresher function\n",
    "renderer = BashRenderer(ROWS,\n",
    "                        COLS,\n",
    "                        clear_fn = lambda: clear_output(wait=True))\n",
    "\n",
    "# Flappy bird environment\n",
    "env = PROJECT_FLAPPY_BIRD_ENV\n",
    "\n",
    "# Our agent\n",
    "agent = StableAgent()\n",
    "\n",
    "# We run a single episode, with rendering, over a maximum of 100 steps\n",
    "run_episode(env,\n",
    "            agent,\n",
    "            max_steps=100,\n",
    "            renderer = renderer,\n",
    "            time_between_frame=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtqXMbKKDWYB"
   },
   "source": [
    "Without rendering now, let's see the average reward we can get over 100 episodes with this agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUbF1hIHDcAN"
   },
   "outputs": [],
   "source": [
    "from deep_rl.project_values import PROJECT_FLAPPY_BIRD_ENV\n",
    "from deep_rl.episode_runner import run_episode\n",
    "\n",
    "# Flappy bird environment\n",
    "env = PROJECT_FLAPPY_BIRD_ENV\n",
    "\n",
    "# Our agent\n",
    "agent = StableAgent()\n",
    "\n",
    "N_EPISODES = 100\n",
    "\n",
    "reward = 0\n",
    "for _ in range(N_EPISODES):\n",
    "  reward+= run_episode(env, agent, max_steps=1000, renderer = None)\n",
    "\n",
    "reward /= N_EPISODES\n",
    "\n",
    "print(f\"Average reward over {N_EPISODES} episodes: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NX9TgKx_EirA"
   },
   "source": [
    "An now, you need to do much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hyzsUZLnpHT"
   },
   "source": [
    "## Let's get to work !\n",
    "\n",
    "Design and train an agent that performs the best possible score on Flappy bird. You can use any method learned in this class. Here are the constraints:\n",
    "- if you chose a Deep learning algorithm, you must use jax and Haiku. Pytorch is not allowed for this project.\n",
    "- your agent should converge in less than an hour. To make sure of that, we will run your code and use whatever checkpoint you have dumped in the given time.\n",
    "- your agent must maximize the reward obtained over 100 episodes with a maximal number of 1000 steps per episode.\n",
    "\n",
    "Do not forget to write **clear and commented code**, you will also be evaluated on that.\n",
    "\n",
    "On top of that, you are asked to plot and analyse the relevant curves showing the evolution of your training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ5dLRk_0YeP"
   },
   "source": [
    "### Agent's API\n",
    "\n",
    "Your agent should implement a method, `sample_action`, which takes two arguments as input, the observed state and wether or not it is in evaluation mode, and pick the action to perform. Appart from that, you can add any other method you want to your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BvoIFd6nrEr"
   },
   "outputs": [],
   "source": [
    "class MyAgent:\n",
    "  \"\"\"Your agent to beat Flappy bird.\"\"\"\n",
    "\n",
    "  def __init__(self, ...):\n",
    "  # Put whatever you want here\n",
    "\n",
    "  def sample_action(self,\n",
    "                    observation: FlappyObs,\n",
    "                    evaluation: bool,\n",
    "                    ) -> int:\n",
    "    \"\"\"Pick the next action to perform\n",
    "\n",
    "    Args:\n",
    "      observation: state of the flappy bird environment.,\n",
    "      evaluation: True if we are in evaluation mode, False if we are training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Your code here !\n",
    "    ...\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gHfxq9_E-zT"
   },
   "source": [
    "## Environment\n",
    "\n",
    "You must use the following flappy bird environment from the deep_rl package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEt5Y82yPw0q"
   },
   "outputs": [],
   "source": [
    "from deep_rl.project_values import PROJECT_FLAPPY_BIRD_ENV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnY-RTgwELdS"
   },
   "source": [
    "### Training loop\n",
    "\n",
    "You can use the following training loop to train your agent. Do not hesitate to play with the different parameters or even modify the code if you think you have a better option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdoO0UY92s2o"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "\n",
    "# Your training loop should perform in less than 2h.\n",
    "MAX_TIME_TRAINING = 3600 * 2\n",
    "\n",
    "@dataclass\n",
    "class EpisodeTrainingStatus:\n",
    "  episode_number: int\n",
    "  reward: float\n",
    "  training_time: float\n",
    "\n",
    "def run_episode_no_rendering(env,\n",
    "                             agent,\n",
    "                             evaluation: bool,\n",
    "                             max_steps: int,\n",
    "                             ) -> float:\n",
    "  \"\"\"Runs a single episode.\n",
    "\n",
    "  Args:\n",
    "    env: environment to consider.\n",
    "    agent: agent to run.\n",
    "    evaluation: if False, will train the agent.\n",
    "    max_steps: number of steps after wich the evaluation should be stoppped\n",
    "      no matter what.\n",
    "  Returns:\n",
    "    The total reward accumulated over the episode.\n",
    "\t\"\"\"\n",
    "\n",
    "\tobservation = env.reset()\n",
    "\ttot_reward = 0\n",
    "\n",
    "\tfor _ in range(max_steps):\n",
    "\n",
    "\t\taction = agent.sample_action(observation, evaluation)\n",
    "\t\tobservation, reward, end_game = env.step(action)\n",
    "\t\ttot_reward += reward\n",
    "\n",
    "\t\tif end_game:\n",
    "\t\t\tbreak\n",
    "\n",
    "\treturn tot_reward\n",
    "\n",
    "def train_agent(env,\n",
    "                agent,\n",
    "                num_episodes: int,\n",
    "                num_eval_episodes: int,\n",
    "                eval_every_N: int,\n",
    "                max_steps_episode: int,\n",
    "                max_time_training: float = MAX_TIME_TRAINING,\n",
    "                ) -> List[EpisodeTrainingStatus]:\n",
    "  \"\"\"Train your agent on the given environment.\n",
    "\n",
    "  Args:\n",
    "    env: environment to consider.\n",
    "    agent: agent to train.\n",
    "    num_episodes: number of episode to run for training.\n",
    "    eval_every_N: frequency at which the agent is evaluated.\n",
    "    max_steps_episode: maximal number of step per episode.\n",
    "    max_time_training: maximal duration of the training loop (in seconds).\n",
    "  Returns:\n",
    "    The total reward accumulated over the episode.\n",
    "\t\"\"\"\n",
    "\n",
    "  all_status = []\n",
    "  print(f\"Episode number:\\t| Average reward on {num_eval_episodes} eval episodes\")\n",
    "  print(\"------------------------------------------------------\")\n",
    "\n",
    "  start_time = time.time()\n",
    "\n",
    "  for episode in range(num_episodes):\n",
    "\n",
    "    run_episode_no_rendering(env,\n",
    "                             agent,\n",
    "                             evaluation=False,\n",
    "                             max_steps=max_steps_episode)\n",
    "\n",
    "    if episode % eval_every_N == 0:\n",
    "      reward=0\n",
    "      d_time = time.time() - start_time\n",
    "      for _ in range(num_eval_episodes):\n",
    "        reward += run_episode(env,\n",
    "                              agent,\n",
    "                              evaluation=True,\n",
    "                              max_steps=max_steps_episode)\n",
    "      reward /= num_eval_episodes\n",
    "      print(f\"\\t{episode}\\t|\\t{reward}\")\n",
    "      all_status.append(EpisodeTrainingStatus(episode_number=episode,\n",
    "                                              reward=reward,\n",
    "                                              training_time=d_time))\n",
    "\n",
    "      if d_time > max_time_training:\n",
    "        break\n",
    "\n",
    "  return all_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sp6QTqEPENaW"
   },
   "source": [
    "### Visualisation\n",
    "\n",
    "You can use the following code to visualize a single run made by your agent. This can help you for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBLse6KIEPZT"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from deep_rl.project_values import PROJECT_FLAPPY_BIRD_ENV\n",
    "from deep_rl.terminal_renderer import BashRenderer\n",
    "from deep_rl.episode_runner import run_episode\n",
    "\n",
    "# Your agent\n",
    "agent = ...\n",
    "\n",
    "# We are going to render the environment !\n",
    "ROWS = 30\n",
    "COLS = 60\n",
    "renderer = BashRenderer(ROWS,\n",
    "                        COLS,\n",
    "                        clear_fn= lambda: clear_output(wait=True))\n",
    "\n",
    "\n",
    "# We run a single episode, with rendering, over a maximum of 1000 steps\n",
    "run_episode(PROJECT_FLAPPY_BIRD_ENV,\n",
    "            agent,\n",
    "            max_steps= 1000,\n",
    "            renderer= renderer,\n",
    "            time_between_frame= 0.1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
